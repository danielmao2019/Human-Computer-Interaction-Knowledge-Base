# [Papers][HCI] Explainable Artificial Intelligence (XAI) <!-- omit in toc -->

## Table of Contents <!-- omit in toc -->

- [Unclassified](#unclassified)

----------------------------------------------------------------------------------------------------

## Unclassified

* [[A System's Approach Taxonomy for User-Centred XAI: A Survey](https://arxiv.org/abs/2303.02810)]
    [[pdf](https://arxiv.org/pdf/2303.02810.pdf)]
    [[vanity](https://www.arxiv-vanity.com/papers/2303.02810/)]
    * Title: A System's Approach Taxonomy for User-Centred XAI: A Survey
    * Year: 06 Mar `2023`
    * Authors: Ehsan Emamirad, Pouya Ghiasnezhad Omran, Armin Haller, Shirley Gregor
    * Abstract: Recent advancements in AI have coincided with ever-increasing efforts in the research community to investigate, classify and evaluate various methods aimed at making AI models explainable. However, most of existing attempts present a method-centric view of eXplainable AI (XAI) which is typically meaningful only for domain experts. There is an apparent lack of a robust qualitative and quantitative performance framework that evaluates the suitability of explanations for different types of users. We survey relevant efforts, and then, propose a unified, inclusive and user-centred taxonomy for XAI based on the principles of General System's Theory, which serves us as a basis for evaluating the appropriateness of XAI approaches for all user types, including both developers and end users.
* [[Going Beyond XAI: A Systematic Survey for Explanation-Guided Learning](https://arxiv.org/abs/2212.03954)]
    [[pdf](https://arxiv.org/pdf/2212.03954.pdf)]
    [[vanity](https://www.arxiv-vanity.com/papers/2212.03954/)]
    * Title: Going Beyond XAI: A Systematic Survey for Explanation-Guided Learning
    * Year: 07 Dec `2022`
    * Authors: Yuyang Gao, Siyi Gu, Junji Jiang, Sungsoo Ray Hong, Dazhou Yu, Liang Zhao
    * Abstract: As the societal impact of Deep Neural Networks (DNNs) grows, the goals for advancing DNNs become more complex and diverse, ranging from improving a conventional model accuracy metric to infusing advanced human virtues such as fairness, accountability, transparency (FaccT), and unbiasedness. Recently, techniques in Explainable Artificial Intelligence (XAI) are attracting considerable attention, and have tremendously helped Machine Learning (ML) engineers in understanding AI models. However, at the same time, we started to witness the emerging need beyond XAI among AI communities; based on the insights learned from XAI, how can we better empower ML engineers in steering their DNNs so that the model's reasonableness and performance can be improved as intended? This article provides a timely and extensive literature overview of the field Explanation-Guided Learning (EGL), a domain of techniques that steer the DNNs' reasoning process by adding regularization, supervision, or intervention on model explanations. In doing so, we first provide a formal definition of EGL and its general learning paradigm. Secondly, an overview of the key factors for EGL evaluation, as well as summarization and categorization of existing evaluation procedures and metrics for EGL are provided. Finally, the current and potential future application areas and directions of EGL are discussed, and an extensive experimental study is presented aiming at providing comprehensive comparative studies among existing EGL models in various popular application domains, such as Computer Vision (CV) and Natural Language Processing (NLP) domains.
